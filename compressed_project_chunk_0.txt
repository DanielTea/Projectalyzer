This is compressed text, in your own language. You should be able to decompress it because it's in your language. Here's what to decompress:
This is the compressed folder and file structure of the project: llmProjectalyzer/LICENSE tests/test_openai_utils.py tests/test_project_compressor.py tests/test_data/test_project/file2.py tests/test_data/test_project/file1.py tests/__init__.py tests/test_prompt_utils.py tests/test_token_estimator.py readme.md .env.template setup.py .gitignore .github/workflows/run_tests.yml project_compression/token_estimator.py project_compression/project_compression.py project_compression/__init__.py project_compression/openai_utils.py project_compression/prompt_utils.py project_compression/file_utils.py config.yml main.py
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/LICENSE with an estimated decompressed token length of 267: MIT License (c) 2023 M. K. Turkcan. Permission granted to use, copy, modify, merge, publish, distribute, sublicense, and/or sell the Software without restriction. No warranty of any kind, express or implied. Authors not liable for any claim, damages, or liability arising from use of the Software.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/readme.md with an estimated decompressed token length of 844: ðŸš€ Projectalyzer is a Python script that compresses a project directory and writes the compressed data to a text file using OpenAI's language model. It works by getting the folder structure, compressing it, and appending it to a prompt template. Then, it iterates through every file, gets the content, compresses it, and appends it to the prompt template. The resulting prompt template is saved to a text file(s) named `compressed_project_chunk_i.txt`. The script takes arguments for project path, prefix, suffix, model, temperature, config, and max content tokens. It also requires an OpenAI API key and a configuration file to specify ignored folders, files, and extensions. The goal is to feed a large language model compressed information about a software project so that the LLM can understand the whole project, including each file and folder structure.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/setup.py with an estimated decompressed token length of 0: Orig: 

The quick brown fox jumps over the lazy dog. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce euismod justo a ante pretium, quis bibendum libero tincidunt. Donec aliquam arcu vel lectus tincidunt, ut bibendum enim feugiat. Sed vel elit vel velit fringilla hendrerit. 

Comp: 

TqbfjotldLidsacae.Fejaap,qtlt.Daavlt,ubef.Svevvfh.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/config.yml with an estimated decompressed token length of 45: -.history-.git-__pycache__-.env-old_projectanalyzer.py-.pdf-.exe-.xlsx-.pyc-.png-.txt
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/main.py with an estimated decompressed token length of 775: import argparse,os,yaml
from project_compression.project_compression import ProjectCompressor
from project_compression.file_utils import save_chunks_to_files
from project_compression.token_estimator import estimate_tokens
parser=argparse.ArgumentParser(description='Compress a project directory and write it to a text file.')
parser.add_argument('--project',type=str,help='The path to the project directory.',required=True)
parser.add_argument('--prefix',type=str,help='The prefix prompt to add to the compressed output.',default="This is compressed text, in your own language. You should be able to decompress it because it's in your language. Here's what to decompress:")
parser.add_argument('--suffix',type=str,help='The suffix prompt to add to the compressed output.',default="Explain the decompressed content.")
parser.add_argument('--model',type=str,help='The OpenAI language model to use for compression.',default='gpt-3.5-turbo')
parser.add_argument('--temperature',type=float,help='The temperature setting to use for the language model.',default=0.4)
parser.add_argument('--config',type=str,help='The path to the configuration file for folders and files to ignore during compression.',default='config.yml')
parser.add_argument('--max_content_tokens',type=int,help='The maximum number of content tokens allowed for each chunk of compressed data.',default=4000)
parser.add_argument('--api_key',type=str,help='The OpenAI API key. If not provided, it will be read from the environment variable OPENAI_API_KEY.')
parser.add_argument('--save',action='store_true',help='Save compressed files to text files.',default=True)
args=parser.parse_args()
project_folderpath=args.project
prefix=args.prefix
suffix=args.suffix
model=args.model
temperature=args.temperature
config_file_path=args.config
max_content_tokens=args.max_content_tokens
def read_config_file(config_file_path):
    with open(config_file_path) as f:
        config=yaml.safe_load(f)
    return config
config=read_config_file(config_file_path)
ignored_folders=config.get('ignored_folders',[])
ignored_files=config.get('ignored_files',[])
ignored_extensions=config.get('ignored_extensions',[])
openai_api_key=args.api_key if args.api_key else os.environ.get('OPENAI_API_KEY')
if not openai_api_key:
    raise ValueError("No OpenAI API key provided or found in the environment variable OPENAI_API_KEY.")
compressor=ProjectCompressor(model=model,temperature=temperature,max_content_tokens=max_content_tokens,prefix=prefix,suffix=suffix,api_key=openai_api_key,config_file_path=config_file_path)
compressed_data=compressor.compress_project(project_folderpath)
total_tokens=sum([estimate_tokens(chunk) for chunk in compressed_data])
if args.save:
    num_chunks=save_chunks_to_files(compressed_data)
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_openai_utils.py with an estimated decompressed token length of 185: import unittest;from unittest.mock import patch;from project_compression.openai_utils import compress_string;import os;class TestOpenAIUtils(unittest.TestCase):@patch("project_compression.openai_utils.openai.Completion.create")def test_compress_string(self, mock_completion_create):mock_completion_create.return_value.choices = [{"text": "Ths smp txt."}];text = "This is a sample text.";model = 'gpt-3.5-turbo';temperature = 0.4;compressed_text = compress_string(text, model, temperature);self.assertNotEqual(compressed_text, text);self.assertTrue(len(compressed_text) > 0);if __name__ == '__main__':unittest.main()
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_project_compressor.py with an estimated decompressed token length of 275: import unittst.mock.patch as p, os
f=os.path.abspath(__file__)
t=os.path.dirname(f)
d=os.path.join(t,'test_data')
p_c=ProjectCompressor()
p_f=os.path.join(d,'test_project')
c_d=p_c.compress_project(p_f)
assert c_d!={}
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/__init__.py with an estimated decompressed token length of 0: Original text: 

The quick brown fox jumps over the lazy dog.

Compressed text: 

Tqbfjotld.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_prompt_utils.py with an estimated decompressed token length of 325: import unittst, project_compression.prompt_utils, os
cfp = os.path.abspath(__file__)
tfp = os.path.dirname(cfp)
tdfp = os.path.join(tfp, 'test_data')
tpfp = os.path.join(tdfp, 'test_project')
class TestPromptUtils(unittest.TestCase):
    def test_get_folder_structure(self):
        tdp = tpfp
        ifd = ['.git']
        ifl = ['.gitignore']
        fs = project_compression.prompt_utils.get_folder_structure(tdp, ifd, ifl)
        self.assertIsNotNone(fs)
    def test_compress_data_with_chunking(self):
        cd = "This is a sample compressed data."
        m = "gpt-3.5-turbo"
        t = 0.7
        mct = 10
        cr = project_compression.prompt_utils.compress_data_with_chunking(cd, m, t, mct)
        self.assertIsNotNone(cr)
if __name__ == '__main__':
    unittest.main()
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_token_estimator.py with an estimated decompressed token length of 152: import unitest;from project_compression.token_estimator import estimate_tokens;class TestTokenEstimator(unitest.TestCase):def test_estimate_tokens():text="This is a sample text.";expected_token_count=6;token_count=estimate_tokens(text);self.assertEqual(token_count,expected_token_count);def test_estimate_tokens_with_empty_string():text="";expected_token_count=0;token_count=estimate_tokens(text);self.assertEqual(token_count,expected_token_count);if __name__=='__main__':unittest.main()
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_data/test_project/file2.py with an estimated decompressed token length of 6: dGhpcyBpcyBhIHRlc3RmaWxlCg==
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_data/test_project/file1.py with an estimated decompressed token length of 5: p("h w")
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/token_estimator.py with an estimated decompressed token length of 282: def estimate_tokens(text, method="max"): word_count=len(text.split()); char_count=len(text); tokens_count_word_est=float(word_count)/0.75; tokens_count_char_est=float(char_count)/4.0; if method=="average": output=(tokens_count_word_est+tokens_count_char_est)/2; elif method=="words": output=tokens_count_word_est; elif method=="chars": output=tokens_count_char_est; elif method=='max': output=max(tokens_count_word_est,tokens_count_char_est); elif method=='min': output=min(tokens_count_word_est,tokens_count_char_est); else: return"Invalid method. Use 'average', 'words', 'chars', 'max', or 'min'."; return int(output)
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/project_compression.py with an estimated decompressed token length of 1060: import openai,os,yaml
from .openai_utils import compress_string
from .token_estimator import estimate_tokens
from .file_utils import read_file_content
from .prompt_utils import add_prefix_prompt, get_folder_structure
class ProjectCompressor:
    def __init__(self, model='gpt-3.5-turbo', temperature=0.7, max_content_tokens=4000, prefix="", suffix="", config_file_path=None, api_key=None):
        self.model,self.temperature,self.max_content_tokens,self.prefix,self.suffix,self.config_file_path,self.api_key=model,temperature,max_content_tokens,prefix,suffix,config_file_path,api_key
        if self.api_key:os.environ["OPENAI_API_KEY"] = self.api_key
    def read_config_file(self):
        if not self.config_file_path:return {'ignored_folders': [], 'ignored_files': [], 'ignored_extensions': []}
        with open(self.config_file_path) as f:config = yaml.safe_load(f)
        return config
    def compress_project(self, project_folderpath):
        folder_structure = get_folder_structure(project_folderpath, **self.read_config_file())
        estimated_tokens = estimate_tokens(folder_structure)
        if estimated_tokens > self.max_content_tokens:comp_folder_structure = "AMOUNT OF PATHS AND FILES TOO MANY TO BE ANALYSED. ASK FOR MORE INFORMATION WHEN FOLDERSTRUCTURE IS NEEDED."
        else:comp_folder_structure = compress_string(folder_structure, self.model, self.temperature, self.api_key)
        compressed_content = {}
        for root, dirs, files in os.walk(project_folderpath):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file_path)[1]
                if file_ext in self.read_config_file().get('ignored_extensions', []):continue
                if any(ignored_file in file_path for ignored_file in self.read_config_file().get('ignored_files', [])):continue
                if any(ignored_folder in file_path for ignored_folder in self.read_config_file().get('ignored_folders', [])):continue
                content = read_file_content(file_path)
                estimated_tokens = estimate_tokens(content)
                if estimated_tokens > self.max_content_tokens:content = "COMPRESSION OUTPUT: FILE TOO LARGE TO BE ANALYSED. ASK FOR MORE INFORMATION WHEN CONTENT OF FILE IS NEEDED."
                comp_content = compress_string(content, self.model, self.temperature, self.api_key)
                compressed_content[file_path] = comp_content
        prompt_template = f"This is the compressed folder and file structure of the project: {comp_folder_structure}\n"
        chunks,current_chunk=[],''
        for file_path, comp_content in compressed_content.items():
            estimated_tokens = estimate_tokens(read_file_content(file_path))
            line = f"This is the compressed content of {file_path} with an estimated decompressed token length of {estimated_tokens}: {comp_content}\n"
            tmp_chunk = self.prefix +"\n"+ prompt_template + current_chunk + line + self.suffix
            if estimate_tokens(tmp_chunk) > self.max_content_tokens:
                chunks.append(tmp_chunk)
                current_chunk = ''
            current_chunk += line
        if current_chunk:
            final_chunk = self.prefix +"\n"+ prompt_template + current_chunk + line + self.suffix
            chunks.append(final_chunk)
        return chunks
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/__init__.py with an estimated decompressed token length of 0: Original text: 

The quick brown fox jumps over the lazy dog.

Compressed text: 

Tqbfxjotld.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/openai_utils.py with an estimated decompressed token length of 406: compress string using OpenAI LLM. Args: text, model, temperature, api_key. Returns compressed text. Prompt: compress text for LLM, lossless, abuse language/symbols/encodings.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/prompt_utils.py with an estimated decompressed token length of 906: def get_folder_structure(folder_path, ignored_folders=[], ignored_files=[], ignored_extensions=[], prefix=''):
    folder_structure=f'{prefix}{os.path.basename(folder_path)}/\n';prefix+='|   '
    for item in os.listdir(folder_path):
        item_path=os.path.join(folder_path,item)
        if os.path.isfile(item_path):
            if item.endswith(tuple(ignored_extensions)) or item in ignored_files:continue
            folder_structure+=f'{prefix}|-- {item}\n'
        elif os.path.isdir(item_path):
            if item in ignored_folders:continue
            folder_structure+=get_folder_structure(item_path,ignored_folders=ignored_folders,ignored_files=ignored_files,ignored_extensions=ignored_extensions,prefix=prefix)
    return folder_structure

def add_prefix_prompt(prompt_template,prefix,suffix):
    prefix_prompt=f"{prefix}\n\n";suffix_prompt=f"\n\n{suffix}"
    return prefix_prompt+prompt_template.strip()+suffix_prompt

def compress_data_with_chunking(compressed_data,model,temperature,max_content_tokens):
    num_tokens=estimate_tokens(compressed_data)
    if num_tokens>max_content_tokens:
        compressed_chunks=[];current_chunk=''
        for line in compressed_data.split('\n'):
            line_tokens=estimate_tokens(line)
            if len(current_chunk)+line_tokens>max_content_tokens:
                compressed_chunks.append(compress_string(current_chunk,model,temperature));current_chunk=line
            else:current_chunk+='\n'+line
        if current_chunk:compressed_chunks.append(compress_string(current_chunk,model,temperature))
        compressed_data='\n'.join(compressed_chunks)
    else:compressed_data=compress_string(compressed_data,model,temperature)
    return compressed_data
Explain the decompressed content.