This is compressed text, in your own language. You should be able to decompress it because it's in your language. Here's what to decompress:
This is the compressed folder and file structure of the project: llmPA/LIC/req.txt/tst/tst_comp.py/tst/__init__.py/tst/tok_est.py/rdme.md/.env.temp/.gitig/proj_comp/tok_est.py/proj_comp.py/__init__.py/openai_utils.py/prompt_utils.py/file_utils.py/cfg.yml/main.py
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/LICENSE with an estimated decompressed token length of 267: MIT License (c) 2023 M. K. Turkcan. Permission granted to use, copy, modify, merge, publish, distribute, sublicense, and/or sell the Software without restriction. No warranty. Authors not liable for any claim, damages, or liability arising from the use of the Software.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/requirements.txt with an estimated decompressed token length of 5: pyenv+dotenv=openai
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/readme.md with an estimated decompressed token length of 844: ğŸš€ Projectalyzer is a Python script that compresses a project directory and writes the compressed data to a text file using OpenAI's language model. It works by getting a text representation of the folder structure using `get_folder_structure()`, compressing it with OpenAI's language model, and iterating through every file in the project directory to get the content of the file, which is also compressed using the language model. The compressed data is saved to a text file named `compressed_project_chunk_i.txt`. To use Projectalyzer, install the required packages, create a copy of the `.env.template` file, navigate to the project directory in the terminal, and run the command `python main.py --project /path/to/project`. The script takes several arguments, including the path to the project directory, the prefix and suffix prompts to add to the compressed output, the OpenAI language model to use for compression, and the maximum number of content tokens allowed for each chunk of compressed data. The `config.yml` file specifies a list of folders and files to ignore during compression. Projectalyzer is licensed under the MIT License.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/config.yml with an estimated decompressed token length of 43: -.history-.git-__pycache__-.env-old_projectanalyzer.py-.pdf-.exe-.xlsx-.pyc-.png
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/main.py with an estimated decompressed token length of 658: import argp,os,yaml,project_compression,save_chunks_to_files,estimate_tokens
p=argp.ArgumentParser();p.add_argument('--project',type=str,help='The path to the project directory.',required=True);p.add_argument('--prefix',type=str,help='The prefix prompt to add to the compressed output.',default="This is compressed text, in your own language. You should be able to decompress it because it's in your language. Here's what to decompress:");p.add_argument('--suffix',type=str,help='The suffix prompt to add to the compressed output.',default="Explain the decompressed content.");p.add_argument('--model',type=str,help='The OpenAI language model to use for compression.',default='gpt-3.5-turbo');p.add_argument('--temperature',type=float,help='The temperature setting to use for the language model.',default=0.2);p.add_argument('--config',type=str,help='The path to the configuration file for folders and files to ignore during compression.',default='config.yml');p.add_argument('--max_content_tokens',type=int,help='The maximum number of content tokens allowed for each chunk of compressed data.',default=4000);a=p.parse_args();project_folderpath=a.project;prefix=a.prefix;suffix=a.suffix;model=a.model;temperature=a.temperature;config_file_path=a.config;max_content_tokens=a.max_content_tokens;def read_config_file(config_file_path):with open(config_file_path) as f:config=yaml.safe_load(f);return config;config=read_config_file(config_file_path);ignored_folders=config.get('ignored_folders',[]);ignored_files=config.get('ignored_files',[]);ignored_extensions=config.get('ignored_extensions',[]);compressor=project_compression.ProjectCompressor(model=model,temperature=temperature,max_content_tokens=max_content_tokens,prefix=prefix,suffix=suffix);compressed_data=compressor.compress_project(project_folderpath,ignored_folders=ignored_folders,ignored_files=ignored_files,ignored_extensions=ignored_extensions);total_tokens=sum([estimate_tokens(chunk) for chunk in compressed_data]);print(f"Estimated total tokensize of compressed_data: {total_tokens}");num_chunks=save_chunks_to_files(compressed_data);print(f"{num_chunks} chunk(s) saved.")
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_compression.py with an estimated decompressed token length of 187: import unittest;from project_compression.project_compression import ProjectCompressor;class TestCompression(unittest.TestCase):def test_compression(self):project_folderpath="test_project";excepted_filetypes=['.pdf','.exe','.xlsx','.pyc','.txt','.png','.env','.history'];compressor=ProjectCompressor(project_folderpath,excepted_filetypes);compressed_data=compressor.compress_project();if __name__=='__main__':unittest.main()
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/__init__.py with an estimated decompressed token length of 0: Orig: 

The quick brown fox jumps over the lazy dog. 

Compressed: 

Tqbfjotld.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_token_estimator.py with an estimated decompressed token length of 115: import unittest;from project_compression.token_estimator import estimate_tokens;class TestTokenEstimator(unittest.TestCase):def test_estimate_tokens(self):text="This is a sample text.";method="max";estimated_tokens=estimate_tokens(text,method);if __name__=='__main__':unittest.main()
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/token_estimator.py with an estimated decompressed token length of 282: def estimate_tokens(text, method="max"): word_count=len(text.split());char_count=len(text);tokens_count_word_est=float(word_count)/0.75;tokens_count_char_est=float(char_count)/4.0;if method=="average":output=(tokens_count_word_est+tokens_count_char_est)/2;elif method=="words":output=tokens_count_word_est;elif method=="chars":output=tokens_count_char_est;elif method=='max':output=max(tokens_count_word_est,tokens_count_char_est);elif method=='min':output=min(tokens_count_word_est,tokens_count_char_est);else:return"Invalid method. Use 'average', 'words', 'chars', 'max', or 'min'.";return int(output)
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/project_compression.py with an estimated decompressed token length of 888: Compress project folder and files, ignore specified folders, files, and extensions. Use given model, temperature, max content tokens, prefix, and suffix. Return compressed chunks.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/__init__.py with an estimated decompressed token length of 0: Orig: 

The quick brown fox jumps over the lazy dog. 

Compressed: 

Tqbfjotld.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/openai_utils.py with an estimated decompressed token length of 344: compress string(text, model, temperature): prompt = "Compress the following text as much as possible in a way that you the LLM can reconstruct exactly 100% the original text. This is for yourself. It does not need to be human readable or understandable. Abuse of language mixing, abbreviations, symbols (unicode and emoji), or any other encodings or internal representations is all permissible, as long as it, if pasted in a new inference cycle, will yield exactly- 100% identical results as the original text. This should be a lossless compression." api_key = os.environ["OPENAI_API_KEY"] openai.api_key = api_key response = openai.ChatCompletion.create(model=model, messages=[{'role': 'user', 'content': prompt + text}], temperature=temperature) return response['choices'][0]['message']['content']
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/prompt_utils.py with an estimated decompressed token length of 885: import os;from project_compression.token_estimator import estimate_tokens;from project_compression.openai_utils import compress_string;def get_folder_structure(folder_path,ignored_folders=[],ignored_files=[],prefix=''):folder_structure=f'{prefix}{os.path.basename(folder_path)}/\n';prefix+='|   ';for item in os.listdir(folder_path):item_path=os.path.join(folder_path,item);if os.path.isfile(item_path):if any([item.endswith(ext)or item==name for ext in ignored_files for name in ignored_files]):continue;folder_structure+=f'{prefix}|-- {item}\n';elif os.path.isdir(item_path):if item in ignored_folders:continue;folder_structure+=get_folder_structure(item_path,ignored_folders=ignored_folders,ignored_files=ignored_files,prefix=prefix);return folder_structure;def add_prefix_prompt(prompt_template,prefix,suffix):prefix_prompt=f"{prefix}\n\n";suffix_prompt=f"\n\n{suffix}";return prefix_prompt+prompt_template.strip()+suffix_prompt;def compress_data_with_chunking(compressed_data,model,temperature,max_content_tokens):num_tokens=estimate_tokens(compressed_data);if num_tokens>max_content_tokens:compressed_chunks=[];current_chunk='';for line in compressed_data.split('\n'):line_tokens=estimate_tokens(line);if len(current_chunk)+line_tokens>max_content_tokens:compressed_chunks.append(compress_string(current_chunk,model,temperature));current_chunk=line;else:current_chunk+='\n'+line;if current_chunk:compressed_chunks.append(compress_string(current_chunk,model,temperature));compressed_data='\n'.join(compressed_chunks);else:compressed_data=compress_string(compressed_data,model,temperature);return compressed_data
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/file_utils.py with an estimated decompressed token length of 198: from project_compression.token_estimator import estimate_tokens;def read_file_content(file_path, encoding='utf-8'):with open(file_path, 'r', encoding=encoding) as f:content = f.read();return content;def save_chunks_to_files(compressed_data, base_filename='compressed_project_chunk'):for i, chunk in enumerate(compressed_data):with open(f"{base_filename}_{i}.txt", "w") as f:f.write(chunk);return len(compressed_data)
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/file_utils.py with an estimated decompressed token length of 198: from project_compression.token_estimator import estimate_tokens;def read_file_content(file_path, encoding='utf-8'):with open(file_path, 'r', encoding=encoding) as f:content = f.read();return content;def save_chunks_to_files(compressed_data, base_filename='compressed_project_chunk'):for i, chunk in enumerate(compressed_data):with open(f"{base_filename}_{i}.txt", "w") as f:f.write(chunk);return len(compressed_data)
Explain the decompressed content.