This is compressed text, in your own language. You should be able to decompress it because it's in your language. Here's what to decompress:
This is the compressed folder and file structure of the project: llmProjAnalyzr/||cmpd_proj_chunk_0.txt||LICENSE||req.txt||tsts/||tst_cmp.py||__init__.py||tst_tok_est.py||readme.md||.env.tmp||.gitignore||proj_cmp/||tok_est.py||proj_cmp.py||__init__.py||openai_utils.py||prompt_utils.py||file_utils.py||config.yml||main.py
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/LICENSE with an estimated decompressed token length of 267: MIT License (c) 2023 M. K. Turkcan. Permission granted to use, copy, modify, merge, publish, distribute, sublicense, and/or sell the Software without restriction. No warranty. Authors not liable for any claim, damages, or liability arising from the use of the Software.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/readme.md with an estimated decompressed token length of 844: ðŸš€ Projectalyzer is a Python script that compresses a project directory and writes the compressed data to a text file using OpenAI's language model. It works by getting a text representation of the folder structure, compressing it, and appending it to a prompt template. Then, it iterates through every file in the project directory, compresses the content of each file, and appends it to the prompt template. The compressed output is saved to a text file(s) named `compressed_project_chunk_i.txt`. To use Projectalyzer, install the required packages, replace `<your_api_key_here>` with your OpenAI API key, navigate to the project directory in the terminal, and run the command `python main.py --project /path/to/project`. The script has several arguments and environment variables that can be configured, and a configuration file can be used to specify folders and files to ignore during compression. The goal of this project is to feed a large language model (LLM) compressed information about a software project so that the LLM can understand the whole project, including each file and folder structure. This project is licensed under the MIT License.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/config.yml with an estimated decompressed token length of 45: -.history-.git-__pycache__-.env-old_projectanalyzer.py-.pdf-.exe-.xlsx-.pyc-.png-.txt
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/main.py with an estimated decompressed token length of 658: import argp,os,yam,pro_com,fil_util,token_est
parser=argp.ArgumentParser()
parser.add_argument('--project',type=str,help='The path to the project directory.',required=True)
parser.add_argument('--prefix',type=str,default="This is compressed text, in your own language. You should be able to decompress it because it's in your language. Here's what to decompress:")
parser.add_argument('--suffix',type=str,default="Explain the decompressed content.")
parser.add_argument('--model',type=str,default='gpt-3.5-turbo')
parser.add_argument('--temperature',type=float,default=0.2)
parser.add_argument('--config',type=str,default='config.yml')
parser.add_argument('--max_content_tokens',type=int,default=4000)
args=parser.parse_args()
project_folderpath=args.project
prefix=args.prefix
suffix=args.suffix
model=args.model
temperature=args.temperature
config_file_path=args.config
max_content_tokens=args.max_content_tokens
def read_config_file(config_file_path):
    with open(config_file_path) as f:
        config=yam.safe_load(f)
    return config
config=read_config_file(config_file_path)
ignored_folders=config.get('ignored_folders',[])
ignored_files=config.get('ignored_files',[])
ignored_extensions=config.get('ignored_extensions',[])
compressor=pro_com.ProjectCompressor(model=model,temperature=temperature,max_content_tokens=max_content_tokens,prefix=prefix,suffix=suffix)
compressed_data=compressor.compress_project(project_folderpath,ignored_folders=ignored_folders,ignored_files=ignored_files,ignored_extensions=ignored_extensions)
total_tokens=sum([token_est.estimate_tokens(chunk) for chunk in compressed_data])
print(f"Estimated total tokensize of compressed_data: {total_tokens}")
num_chunks=fil_util.save_chunks_to_files(compressed_data)
print(f"{num_chunks} chunk(s) saved.")
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_compression.py with an estimated decompressed token length of 187: imp unittest;from project_compression.project_compression import ProjectCompressor;class TestCompression(unittest.TestCase):def test_compression():project_folderpath="test_project";excepted_filetypes=['.pdf','.exe','.xlsx','.pyc','.txt','.png','.env','.history'];compressor=ProjectCompressor(project_folderpath,excepted_filetypes);compressed_data=compressor.compress_project();if __name__=='__main__':unittest.main()
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/__init__.py with an estimated decompressed token length of 0: Original text: 

The quick brown fox jumps over the lazy dog.

Compressed text: 

Tqbfxjotld.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_token_estimator.py with an estimated decompressed token length of 115: import unittest;from project_compression.token_estimator import estimate_tokens;class TestTokenEstimator(unittest.TestCase):def test_estimate_tokens(self):text="This is a sample text.";method="max";estimated_tokens=estimate_tokens(text,method);if __name__=='__main__':unittest.main()
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/token_estimator.py with an estimated decompressed token length of 282: def estimate_tokens(text, method="max"): word_count=len(text.split());char_count=len(text);tokens_count_word_est=float(word_count)/0.75;tokens_count_char_est=float(char_count)/4.0;if method=="average":output=(tokens_count_word_est+tokens_count_char_est)/2;elif method=="words":output=tokens_count_word_est;elif method=="chars":output=tokens_count_char_est;elif method=='max':output=max(tokens_count_word_est,tokens_count_char_est);elif method=='min':output=min(tokens_count_word_est,tokens_count_char_est);else:return"Invalid method. Use 'average', 'words', 'chars', 'max', or 'min'.";return int(output)
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/project_compression.py with an estimated decompressed token length of 888: import os, .openai_utils, .token_estimator, .file_utils, .prompt_utils
class ProjectCompressor:
    def __init__(self, model='gpt-3.5-turbo', temperature=0.7, max_tokens=4000, prefix="", suffix=""):
        self.model, self.temperature, self.max_tokens, self.prefix, self.suffix = model, temperature, max_tokens, prefix, suffix

    def compress_project(self, project_folderpath, ignored_folders=[], ignored_files=[], ignored_extensions=[]):
        folder_structure = .prompt_utils.get_folder_structure(project_folderpath, ignored_folders=ignored_folders, ignored_files=ignored_files)
        estimated_tokens = .token_estimator.estimate_tokens(folder_structure)
        if estimated_tokens > self.max_tokens:
            content = "AMOUNT OF PATHS AND FILES TOO MANY TO BE ANALYSED. ASK FOR MORE INFORMATION WHEN FOLDERSTRUCTURE IS NEEDED."
        comp_folder_structure = .openai_utils.compress_string(folder_structure, self.model, self.temperature)
        compressed_content = {}
        for root, _, files in os.walk(project_folderpath):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file_path)[1]
                if file_ext in ignored_extensions or any(ignored_file in file_path for ignored_file in ignored_files) or any(ignored_folder in file_path for ignored_folder in ignored_folders):
                    continue
                content = .file_utils.read_file_content(file_path)
                estimated_tokens = .token_estimator.estimate_tokens(content)
                if estimated_tokens > self.max_tokens:
                    content = "COMPRESSION OUTPUT: FILE TOO LARGE TO BE ANALYSED. ASK FOR MORE INFORMATION WHEN CONTENT OF FILE IS NEEDED."
                comp_content = .openai_utils.compress_string(content, self.model, self.temperature)
                compressed_content[file_path] = comp_content
        prompt_template, chunks, current_chunk = f"This is the compressed folder and file structure of the project: {comp_folder_structure}\n", [], ''
        for file_path, comp_content in compressed_content.items():
            estimated_tokens = .token_estimator.estimate_tokens(.file_utils.read_file_content(file_path))
            line = f"This is the compressed content of {file_path} with an estimated decompressed token length of {estimated_tokens}: {comp_content}\n"
            tmp_chunk = self.prefix +"\n"+ prompt_template + current_chunk + line + self.suffix
            if .token_estimator.estimate_tokens(tmp_chunk) > self.max_tokens:
                chunks.append(tmp_chunk)
                current_chunk = ''
            current_chunk += line
        if current_chunk:
            final_chunk = self.prefix +"\n"+ prompt_template + current_chunk + line + self.suffix
            chunks.append(final_chunk)
        return chunks
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/__init__.py with an estimated decompressed token length of 0: Orig: 

The quick brown fox jumps over the lazy dog. 

Compressed: 

Tqbfjotld.
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/openai_utils.py with an estimated decompressed token length of 344: compress string openai os dotenv load_dotenv text model temperature prompt api_key create role user content message
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/prompt_utils.py with an estimated decompressed token length of 885: import os;from project_compression.token_estimator import estimate_tokens;from project_compression.openai_utils import compress_string;def get_folder_structure(folder_path,ignored_folders=[],ignored_files=[],prefix=''):folder_structure=f'{prefix}{os.path.basename(folder_path)}/\n';prefix+='|   ';for item in os.listdir(folder_path):item_path=os.path.join(folder_path,item);if os.path.isfile(item_path):if any([item.endswith(ext)or item==name for ext in ignored_files for name in ignored_files]):continue;folder_structure+=f'{prefix}|-- {item}\n';elif os.path.isdir(item_path):if item in ignored_folders:continue;folder_structure+=get_folder_structure(item_path,ignored_folders=ignored_folders,ignored_files=ignored_files,prefix=prefix);return folder_structure;def add_prefix_prompt(prompt_template,prefix,suffix):prefix_prompt=f"{prefix}\n\n";suffix_prompt=f"\n\n{suffix}";return prefix_prompt+prompt_template.strip()+suffix_prompt;def compress_data_with_chunking(compressed_data,model,temperature,max_content_tokens):num_tokens=estimate_tokens(compressed_data);if num_tokens>max_content_tokens:compressed_chunks=[];current_chunk='';for line in compressed_data.split('\n'):line_tokens=estimate_tokens(line);if len(current_chunk)+line_tokens>max_content_tokens:compressed_chunks.append(compress_string(current_chunk,model,temperature));current_chunk=line;else:current_chunk+='\n'+line;if current_chunk:compressed_chunks.append(compress_string(current_chunk,model,temperature));compressed_data='\n'.join(compressed_chunks);else:compressed_data=compress_string(compressed_data,model,temperature);return compressed_data
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/file_utils.py with an estimated decompressed token length of 198: from project_compression.token_estimator import estimate_tokens;def read_file_content(file_path, encoding='utf-8'):with open(file_path, 'r', encoding=encoding) as f:content = f.read();return content;def save_chunks_to_files(compressed_data, base_filename='compressed_project_chunk'):for i, chunk in enumerate(compressed_data):with open(f"{base_filename}_{i}.txt", "w") as f:f.write(chunk);return len(compressed_data)
This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/file_utils.py with an estimated decompressed token length of 198: from project_compression.token_estimator import estimate_tokens;def read_file_content(file_path, encoding='utf-8'):with open(file_path, 'r', encoding=encoding) as f:content = f.read();return content;def save_chunks_to_files(compressed_data, base_filename='compressed_project_chunk'):for i, chunk in enumerate(compressed_data):with open(f"{base_filename}_{i}.txt", "w") as f:f.write(chunk);return len(compressed_data)
Explain the decompressed content.