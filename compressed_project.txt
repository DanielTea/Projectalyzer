This is compressed text, in your own language. You should be able to decompress it because it's in your language. Here's what to decompress:

        This is the compressed folder and file structure of the project: llmProjAnalyzer: compressed_project_prompt.txt, LICENSE, requirements.txt, tests/test_compression.py, tests/__init__.py, tests/test_token_estimator.py, compressed_project.txt, readme.md, .env.template, .gitignore, project_compression/token_estimator.py, project_compression/project_compression.py, project_compression/__init__.py, project_compression/openai_utils.py, project_compression/prompt_utils.py, project_compression/file_utils.py, config.yml, main.py.
        
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/LICENSE with an estimated decompressed token length of 267: MIT License (c) 2023 M. K. Turkcan. Permission granted to use, copy, modify, merge, publish, distribute, sublicense, and/or sell the Software without restriction. No warranty provided. Authors not liable for any claim, damages, or liability.
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/readme.md with an estimated decompressed token length of 755: ðŸš€ Projectalyzer compresses a project directory to a text file using OpenAI's language model. It works by getting the folder structure, compressing it, and appending it to a prompt template. Then, it iterates through every file in the directory, compresses the content, and appends it to the prompt template. The resulting file is saved as `compressed_project_prompt.txt`. Usage involves installing required packages, creating an `.env` file with an OpenAI API key, navigating to the project directory, and running the command `python projectalyzer.py --project /path/to/project`. Optional arguments include `--prefix`, `--suffix`, `--model`, `--temperature`, and `--excepted_filetypes`. The goal of Projectalyzer is to feed a large language model compressed information about a software project so that it can understand the whole project. This project is licensed under the MIT License.
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/config.yml with an estimated decompressed token length of 45: -.history-.git-__pycache__-.env-old_projectanalyzer.py-.pdf-.exe-.xlsx-.pyc-.txt-.png
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/main.py with an estimated decompressed token length of 711: import argparse, os, yaml, project_compression.project_compression, project_compression.openai_utils, project_compression.prompt_utils

parser=argparse.ArgumentParser();parser.add_argument('--project',type=str,help='The path to the project directory.',required=True);parser.add_argument('--prefix',type=str,help='The prefix prompt to add to the compressed output.',default="This is compressed text, in your own language. You should be able to decompress it because it's in your language. Here's what to decompress:");parser.add_argument('--suffix',type=str,help='The suffix prompt to add to the compressed output.',default="Explain the decompressed content.");parser.add_argument('--model',type=str,help='The OpenAI language model to use for compression.',default='gpt-3.5-turbo');parser.add_argument('--temperature',type=float,help='The temperature setting to use for the language model.',default=0.2);parser.add_argument('--config',type=str,help='The path to the configuration file for folders and files to ignore during compression.',default='config.yml');parser.add_argument('--max_content_tokens',type=int,help='The maximum number of content tokens allowed for each chunk of compressed data.',default=4000);parser.add_argument('--double_comp',type=bool,help='Whether to compress the compressed output again or not.',default=False);args=parser.parse_args();project_folderpath=args.project;prefix=args.prefix;suffix=args.suffix;model=args.model;temperature=args.temperature;config_file_path=args.config;max_content_tokens=args.max_content_tokens;double_comp=args.double_comp

def read_config_file(config_file_path):
    with open(config_file_path) as f:
        config = yaml.safe_load(f)
    return config

config = read_config_file(config_file_path)

ignored_folders = config.get('ignored_folders', [])
ignored_files = config.get('ignored_files', [])
ignored_extensions = config.get('ignored_extensions', [])

compressor = project_compression.project_compression.ProjectCompressor(model=model, temperature=temperature, max_content_tokens=max_content_tokens)

compressed_data = compressor.compress_project(project_folderpath, ignored_folders=ignored_folders, ignored_files=ignored_files, ignored_extensions=ignored_extensions)

if double_comp:
    compressed_data = project_compression.prompt_utils.compress_data_with_chunking(compressed_data, model, temperature, max_content_tokens)

with open('compressed_project.txt', 'w') as f:
    f.write(f"{prefix}\n{compressed_data}\n{suffix}")

print("Compressed project saved to compressed_project.txt.")
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_compression.py with an estimated decompressed token length of 187: import unittest;from project_compression.project_compression import ProjectCompressor;class TestCompression(unittest.TestCase):def test_compression(self):project_folderpath="test_project";excepted_filetypes=['.pdf','.exe','.xlsx','.pyc','.txt','.png','.env','.history'];compressor=ProjectCompressor(project_folderpath,excepted_filetypes);compressed_data=compressor.compress_project();if __name__=='__main__':unittest.main()
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/__init__.py with an estimated decompressed token length of 0: Original text: 

The quick brown fox jumps over the lazy dog.

Compressed text: 

Tqbfjotld.
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/tests/test_token_estimator.py with an estimated decompressed token length of 115: import unittest;from project_compression.token_estimator import estimate_tokens;class TestTokenEstimator(unittest.TestCase):def test_estimate_tokens(self):text="This is a sample text.";method="max";estimated_tokens=estimate_tokens(text,method);if __name__=='__main__':unittest.main()
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/token_estimator.py with an estimated decompressed token length of 282: def estimate_tokens(text, method="max"): word_count = len(text.split()); char_count = len(text); tokens_count_word_est = float(word_count) / 0.75; tokens_count_char_est = float(char_count) / 4.0; output = (tokens_count_word_est + tokens_count_char_est) / 2 if method == "average" else tokens_count_word_est if method == "words" else tokens_count_char_est if method == 'chars' else max(tokens_count_word_est, tokens_count_char_est) if method == 'max' else min(tokens_count_word_est, tokens_count_char_est) if method == 'min' else "Invalid method. Use 'average', 'words', 'chars', 'max', or 'min'."; return int(output)
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/project_compression.py with an estimated decompressed token length of 734: import os, .openai_utils.compress_string, .token_estimator.estimate_tokens, .file_utils.read_file_content, .prompt_utils.add_prefix_prompt, .prompt_utils.get_folder_structure

class ProjectCompressor:
    __init__(model='gpt-3.5-turbo', temperature=0.7, max_content_tokens=4000)
        self.model = model, self.temperature = temperature, self.max_content_tokens = max_content_tokens

    compress_project(project_folderpath, ignored_folders=[], ignored_files=[], ignored_extensions=[])
        folder_structure = get_folder_structure(project_folderpath, ignored_folders=ignored_folders, ignored_files=ignored_files)
        estimated_tokens = estimate_tokens(folder_structure)
        if estimated_tokens > self.max_content_tokens:
            content = "AMOUNT OF PATHS AND FILES TOO MANY TO BE ANALYSED. ASK FOR MORE INFORMATION WHEN FOLDERSTRUCTURE IS NEEDED."
        comp_folder_structure = compress_string(folder_structure, self.model, self.temperature)
        compressed_content = {}
        for root, _, files in os.walk(project_folderpath):
            for file in files:
                file_path = os.path.join(root, file)
                file_ext = os.path.splitext(file_path)[1]
                if file_ext in ignored_extensions:
                    continue
                if any(ignored_file in file_path for ignored_file in ignored_files):
                    continue
                if any(ignored_folder in file_path for ignored_folder in ignored_folders):
                    continue
                content = read_file_content(file_path)
                estimated_tokens = estimate_tokens(content)
                if estimated_tokens > self.max_content_tokens:
                    content = "FILE TOO LARGE TO BE ANALYSED. ASK FOR MORE INFORMATION WHEN CONTENT OF FILE IS NEEDED."
                comp_content = compress_string(content, self.model, self.temperature)
                compressed_content[file_path] = comp_content
        prompt_template = f"""This is the compressed folder and file structure of the project: {comp_folder_structure}"""
        for file_path, comp_content in compressed_content.items():
            estimated_tokens = estimate_tokens(read_file_content(file_path))
            prompt_template += f"""This is the compressed content of {file_path} with an estimated decompressed token length of {estimated_tokens}: {comp_content}"""
        return prompt_template
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/__init__.py with an estimated decompressed token length of 0: Orig: 

The quick brown fox jumps over the lazy dog. 

Compressed: 

Tqbfjotld.
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/openai_utils.py with an estimated decompressed token length of 344: compress string using openai language model to compress text. model name and temperature setting are required. returns compressed text.
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/prompt_utils.py with an estimated decompressed token length of 885: import os;from project_compression.token_estimator import estimate_tokens;from project_compression.openai_utils import compress_string;def get_folder_structure(folder_path,ignored_folders=[],ignored_files=[],prefix=''):folder_structure=f'{prefix}{os.path.basename(folder_path)}/\n';prefix+='|   ';for item in os.listdir(folder_path):item_path=os.path.join(folder_path,item);if os.path.isfile(item_path):if any([item.endswith(ext)or item==name for ext in ignored_files for name in ignored_files]):continue;folder_structure+=f'{prefix}|-- {item}\n';elif os.path.isdir(item_path):if item in ignored_folders:continue;folder_structure+=get_folder_structure(item_path,ignored_folders=ignored_folders,ignored_files=ignored_files,prefix=prefix);return folder_structure;def add_prefix_prompt(prompt_template,prefix,suffix):prefix_prompt=f"{prefix}\n\n";suffix_prompt=f"\n\n{suffix}";return prefix_prompt+prompt_template.strip()+suffix_prompt;def compress_data_with_chunking(compressed_data,model,temperature,max_content_tokens):num_tokens=estimate_tokens(compressed_data);if num_tokens>max_content_tokens:compressed_chunks=[];current_chunk='';for line in compressed_data.split('\n'):line_tokens=estimate_tokens(line);if len(current_chunk)+line_tokens>max_content_tokens:compressed_chunks.append(compress_string(current_chunk,model,temperature));current_chunk=line;else:current_chunk+='\n'+line;if current_chunk:compressed_chunks.append(compress_string(current_chunk,model,temperature));compressed_data='\n'.join(compressed_chunks);else:compressed_data=compress_string(compressed_data,model,temperature);return compressed_data
            
            This is the compressed content of /Users/danieltremer/Documents/llmProjectalyzer/project_compression/file_utils.py with an estimated decompressed token length of 117: def read_file_content(fp, enc='utf-8'):with open(fp,'r',encoding=enc) as f:return f.read()
            
Explain the decompressed content.